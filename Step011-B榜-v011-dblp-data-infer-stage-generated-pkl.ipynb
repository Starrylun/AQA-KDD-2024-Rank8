{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "3a613855-5739-4583-bd34-65bdeb2e53e6",
   "metadata": {},
   "source": [
    "# pre lib "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "cdbb43ab-c241-46e1-833c-adb347d6ba4d",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "import os\n",
    "os.environ['CUDA_VISIBLE_DEVICES'] = '0,2'\n",
    "\n",
    "import warnings \n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "from IPython.core.interactiveshell import InteractiveShell\n",
    "InteractiveShell.ast_node_interactivity = 'all'\n",
    "\n",
    "\n",
    "## 查看 dict 的 前三个内容 \n",
    "import itertools\n",
    "def get_first_three_from_dict(my_dict): \n",
    "    # 转换字典为迭代器\n",
    "    iter_dict = iter(my_dict.items())\n",
    "    # 使用 islice 取出前三个元素\n",
    "    first_three = dict(itertools.islice(iter_dict, 3))\n",
    "    return first_three"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c77f2957-b10a-40b7-8e5d-7ebfd19929d3",
   "metadata": {},
   "source": [
    "# data lib "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "bddb1eba-f126-4c2f-8912-b319ceb668ed",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "from IPython.core.interactiveshell import InteractiveShell \n",
    "InteractiveShell.ast_node_interactivity = 'all'\n",
    "\n",
    "import glob\n",
    "import pandas as pd\n",
    "import json\n",
    "import re \n",
    "\n",
    "import random \n",
    "random.seed(0)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "32b70d29-5297-40df-9a05-5ce2592e8634",
   "metadata": {},
   "source": [
    "# 文件读取和清洗的参数"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "3fef457b-a34e-4cc8-8954-02b4a451fb61",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "70575"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "text/plain": [
       "question    0\n",
       "body        0\n",
       "pids        0\n",
       "dtype: int64"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "text/plain": [
       "question    0\n",
       "body        0\n",
       "dtype: int64"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "text/plain": [
       "pids        0\n",
       "title       3\n",
       "abstract    0\n",
       "dtype: int64"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "## 读取数据的内容 \n",
    "def read_train_valid_test(path): \n",
    "    data = []\n",
    "    \n",
    "    assert path.endswith('.txt')\n",
    "    # 打开并逐行读取txt文件\n",
    "    with open(path, 'r') as f:\n",
    "        for line in f:\n",
    "            # 使用json.loads将每一行转换为字典\n",
    "            data.append(json.loads(line))\n",
    "            \n",
    "    ## 转换成 df 格式 \n",
    "    data = pd.DataFrame(data)\n",
    "    return data \n",
    "\n",
    "def read_json_to_df(json_path): \n",
    "    # 打开json文件\n",
    "    with open(json_path, 'r') as file:\n",
    "        # 解析json文件\n",
    "        data = json.load(file)\n",
    "    ## json --> df \n",
    "    data = pd.DataFrame(data).T.reset_index(names=['pids'])\n",
    "    return data\n",
    "\n",
    "## 清洗数据 \n",
    "def clean_body_remove_symbol(text): \n",
    "    ## clean_body_remove_symbol(text) \n",
    "    text = re.sub('<[^<]+?>', ' ', text).replace('\\n', '').strip()\n",
    "    text = re.sub(r'\\s+', ' ', text)\n",
    "    text = text.replace('http://', '').replace('https://', '').replace('.com', '').replace('.cn', '')\n",
    "    return text \n",
    "\n",
    "## data\n",
    "trpath = 'data/AQA/qa_train.txt'\n",
    "train = read_train_valid_test(trpath)\n",
    "\n",
    "valpath = 'data/AQA/qa_valid_wo_ans.txt'\n",
    "valid = read_train_valid_test(valpath)\n",
    "\n",
    "testpath = 'data/AQA-test-public/qa_test_wo_ans_new.txt'\n",
    "test = read_train_valid_test(testpath)\n",
    "\n",
    "## json  \n",
    "json_path = 'data/AQA/pid_to_title_abs_new.json'\n",
    "df_json_old = read_json_to_df(json_path) \n",
    "\n",
    "json_path = 'data/AQA-test-public/pid_to_title_abs_update_filter.json'\n",
    "df_json_new = read_json_to_df(json_path) \n",
    "\n",
    "len(set(df_json_new['pids']).difference(set(df_json_old['pids'])))\n",
    "df_json = pd.merge(df_json_new, df_json_old, how='outer', on=['pids', 'title', 'abstract'])\n",
    "\n",
    "del df_json_new, df_json_old\n",
    "\n",
    "##\n",
    "train.isnull().sum()\n",
    "valid.isnull().sum()\n",
    "df_json.isnull().sum()\n",
    "\n",
    "## train 中的 body 内容给定 \n",
    "train['body'] = train['body'].apply(clean_body_remove_symbol)\n",
    "valid['body'] = valid['body'].apply(clean_body_remove_symbol)\n",
    "test['body'] = test['body'].apply(clean_body_remove_symbol)\n",
    "\n",
    "## passage 文章清洗 \n",
    "df_json['title'] = df_json['title'].fillna('None').apply(clean_body_remove_symbol) \n",
    "df_json['abstract'] = df_json['abstract'].apply(clean_body_remove_symbol) \n",
    "\n",
    "df_json = df_json.reset_index()\n",
    "df_json = df_json.rename(columns={'index':'id'}) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "c56c775d-ca80-424b-9888-278373ff1dc4",
   "metadata": {
    "scrolled": true,
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>question</th>\n",
       "      <th>body</th>\n",
       "      <th>pids</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Why would it ever be possible for Java to be f...</td>\n",
       "      <td>Sometimes Java outperforms C++ in benchmarks. ...</td>\n",
       "      <td>[619bb02b1c45e57ce901d5f1]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Are stacks the only reasonable way to structur...</td>\n",
       "      <td>Most architectures I've seen rely on a call st...</td>\n",
       "      <td>[53e99876b7602d97020b053d]</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                            question  \\\n",
       "0  Why would it ever be possible for Java to be f...   \n",
       "1  Are stacks the only reasonable way to structur...   \n",
       "\n",
       "                                                body  \\\n",
       "0  Sometimes Java outperforms C++ in benchmarks. ...   \n",
       "1  Most architectures I've seen rely on a call st...   \n",
       "\n",
       "                         pids  \n",
       "0  [619bb02b1c45e57ce901d5f1]  \n",
       "1  [53e99876b7602d97020b053d]  "
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>question</th>\n",
       "      <th>body</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>How is this Pytorch expression equivalent to t...</td>\n",
       "      <td>I found the following PyTorch code (from this ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Why do universities have to spend money on jou...</td>\n",
       "      <td>Obviously this is a question in the light of t...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                            question  \\\n",
       "0  How is this Pytorch expression equivalent to t...   \n",
       "1  Why do universities have to spend money on jou...   \n",
       "\n",
       "                                                body  \n",
       "0  I found the following PyTorch code (from this ...  \n",
       "1  Obviously this is a question in the light of t...  "
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>question</th>\n",
       "      <th>body</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Cloud Computing - Suggesting customers migrate...</td>\n",
       "      <td>AN OVERBROAD PATENT ON suggesting customer mig...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Requesting prior art on Google machine learnin...</td>\n",
       "      <td>Google is attempting to patent well known conc...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                            question  \\\n",
       "0  Cloud Computing - Suggesting customers migrate...   \n",
       "1  Requesting prior art on Google machine learnin...   \n",
       "\n",
       "                                                body  \n",
       "0  AN OVERBROAD PATENT ON suggesting customer mig...  \n",
       "1  Google is attempting to patent well known conc...  "
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>id</th>\n",
       "      <th>pids</th>\n",
       "      <th>title</th>\n",
       "      <th>abstract</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0</td>\n",
       "      <td>5390877920f70186a0d2cb29</td>\n",
       "      <td>A New Use Of An Automated Reasoning Assistant ...</td>\n",
       "      <td>The field of automated reasoning is an outgrow...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1</td>\n",
       "      <td>5390877920f70186a0d2cc14</td>\n",
       "      <td>Why AM an EUISKO appear to work.</td>\n",
       "      <td>Seven years ago, the AM program was constructe...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   id                      pids  \\\n",
       "0   0  5390877920f70186a0d2cb29   \n",
       "1   1  5390877920f70186a0d2cc14   \n",
       "\n",
       "                                               title  \\\n",
       "0  A New Use Of An Automated Reasoning Assistant ...   \n",
       "1                   Why AM an EUISKO appear to work.   \n",
       "\n",
       "                                            abstract  \n",
       "0  The field of automated reasoning is an outgrow...  \n",
       "1  Seven years ago, the AM program was constructe...  "
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train.head(2)\n",
    "valid.head(2)\n",
    "test.head(2)\n",
    "df_json.head(2)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "55552f96-6a01-422a-a983-678ea2a22336",
   "metadata": {},
   "source": [
    "# Create candidates - Retrival for Training and Valid"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "91a8922d-942c-48aa-9d52-6f54cfc325c4",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "import os\n",
    "os.environ['CUDA_VISIBLE_DEVICES'] = '0,2'\n",
    "\n",
    "import ctypes\n",
    "import gc\n",
    "import torch\n",
    "import faiss\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from tqdm.auto import tqdm\n",
    "from nltk.translate.bleu_score import sentence_bleu, SmoothingFunction\n",
    "\n",
    "from torch.utils.data import DataLoader\n",
    "from transformers import (\n",
    "    BatchEncoding, AutoModelForMultipleChoice,\n",
    "    AutoTokenizer, PreTrainedTokenizer,\n",
    "    AutoModelForSequenceClassification,\n",
    "    AutoModel, PreTrainedModel,\n",
    "    AutoConfig, AutoModelForCausalLM\n",
    ")\n",
    "from sentence_transformers import SentenceTransformer\n",
    "from accelerate import init_empty_weights\n",
    "from accelerate.utils.modeling import set_module_tensor_to_device\n",
    "from safetensors.torch import load_file\n",
    "\n",
    "## 用于 topK 生成 \n",
    "import math\n",
    "import pickle\n",
    "import multiprocessing\n",
    "\n",
    "from typing import List, Tuple, Dict, Union\n",
    "from concurrent.futures import ThreadPoolExecutor\n",
    "\n",
    "## add \n",
    "import time\n",
    "import itertools\n",
    "\n",
    "## \n",
    "def clean_memory():\n",
    "    gc.collect()\n",
    "    ctypes.CDLL(\"libc.so.6\").malloc_trim(0)\n",
    "    torch.cuda.empty_cache()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "af6a92d9-bd59-4dbc-b301-c23bded0e6fb",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "os.makedirs('outslgb', exist_ok=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "aa6b7ebc-c88e-4682-a046-96c1b5c1df1e",
   "metadata": {},
   "source": [
    "## MiniLM-6"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2063234b-12d7-4e7a-8d72-43b0587ec75d",
   "metadata": {},
   "source": [
    "### encode question "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "1a7a5ef0-ad1b-4df3-a112-b13e09ef7c05",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "def encode_questions_minilm(args):\n",
    "    \"\"\"Using all-MiniLM-L6-v2 model,\n",
    "    Generate embeddings for concatenation of prompt and answer options.\n",
    "    It is supposed to run in 2 processes, each embeds half of the test dataframe.\n",
    "    Result as float16 numpy array [len(df) x 384] is saved in pkl file.\n",
    "\n",
    "    Args:\n",
    "        args:  \n",
    "            minilm_name (str) \n",
    "            proc_id (int): number of process we are in.\n",
    "    \"\"\"\n",
    "    minilm_name, proc_id, df = args\n",
    "    \n",
    "    if minilm_name == 'minilmL12': \n",
    "        model_name = '/mntdata/wangql43/A000Files/A003Model/bensonpeng/all-MiniLM-L12-v2/'\n",
    "    elif minilm_name == 'minilm': \n",
    "        model_name = '/mntdata/wangql43/A000Files/A003Model/bensonpeng/all-MiniLM-L6-v2/'\n",
    "    elif minilm_name == 'paraphrase': \n",
    "        model_name = '/mntdata/wangql43/A000Files/A003Model/bensonpeng/paraphrase-MiniLM-L12-v2/'\n",
    "        \n",
    "    model = SentenceTransformer(model_name, trust_remote_code=True)\n",
    "    \n",
    "    # valpath = 'data/AQA/qa_valid_wo_ans.txt'\n",
    "    # df = read_train_valid_test(valpath)\n",
    "    # df['body'] = df['body'].apply(clean_body_remove_symbol)\n",
    "    \n",
    "    df = np.array_split(df, 2)[proc_id]\n",
    "    texts = []\n",
    "    for _, row in df.iterrows():\n",
    "        text = f\"{row.question}\\n{row.body}\"\n",
    "        texts.append(text)\n",
    "    embs = model.encode(texts, device=f'cuda:{proc_id}', batch_size=256).astype(np.float16)\n",
    "    \n",
    "    with open(f'outslgb/encoded_questions_{minilm_name}_{df_name}_{proc_id}.pkl', 'wb') as f:\n",
    "        pickle.dump(embs, f)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dc6e213f-0ebc-400c-8f5e-f4dce179c0da",
   "metadata": {},
   "source": [
    "### combine question embeddings "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "d30b3bb9-a8b1-41c0-bc81-750ecabaa199",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "def combine_embs_from_processes(model_name: str):\n",
    "    \"\"\"Question encoding functions produce two embeddings files for each model,\n",
    "    this one combines two into one. \n",
    "    Args:\n",
    "        model_name (str): 'bge' or 'minilm'.\n",
    "    \"\"\"\n",
    "    embs = []\n",
    "    for proc_idx in range(2):\n",
    "        with open(f'outslgb/encoded_questions_{model_name}_{df_name}_{proc_idx}.pkl', 'rb') as f:\n",
    "            embs.append(pickle.load(f))\n",
    "    embs = np.concatenate(embs, axis=0)\n",
    "    with open(f'outslgb/encoded_questions_{model_name}_{df_name}.pkl', 'wb') as f:\n",
    "        pickle.dump(embs, f) "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "535656d9-115e-48bc-967f-ad1635724fea",
   "metadata": {},
   "source": [
    "### test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "ab8d00a2-76d5-49b5-a396-0ee422d0e7a9",
   "metadata": {
    "scrolled": true,
    "tags": []
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of the model checkpoint at /mntdata/wangql43/A000Files/A003Model/bensonpeng/all-MiniLM-L6-v2/ were not used when initializing BertModel: ['classifier.bias', 'classifier.weight']\n",
      "- This IS expected if you are initializing BertModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing BertModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
      "Some weights of the model checkpoint at /mntdata/wangql43/A000Files/A003Model/bensonpeng/all-MiniLM-L6-v2/ were not used when initializing BertModel: ['classifier.bias', 'classifier.weight']\n",
      "- This IS expected if you are initializing BertModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing BertModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[None, None, None, None, None, None]"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "text/plain": [
       "[None, None, None]"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "MiniLM-L6-L12-paraphrase-v2 encoding: 12.403230905532837 s\n"
     ]
    }
   ],
   "source": [
    "# # MiniLM-L6-L12-paraphrase \n",
    "start = time.time()\n",
    "global df_name\n",
    "df_name = 'test'\n",
    "\n",
    "params_list = list(itertools.product(['minilmL12', 'minilm', 'paraphrase'], [0,1], [test]))\n",
    "with multiprocessing.Pool(len(params_list)) as pool:\n",
    "    pool.map(encode_questions_minilm, params_list) \n",
    "    \n",
    "with multiprocessing.Pool(3) as pool:\n",
    "    pool.map(combine_embs_from_processes, [('minilmL12'), ('minilm'), ('paraphrase')])\n",
    "    \n",
    "print(f'MiniLM-L6-L12-paraphrase-v2 encoding: {time.time() - start} s')\n",
    "del df_name"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3991f22f-362b-4520-b922-163369f56552",
   "metadata": {},
   "source": [
    "### valid "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "f0e63478-b524-463c-b7ee-6e0c91561b72",
   "metadata": {
    "scrolled": true,
    "tags": []
   },
   "outputs": [],
   "source": [
    "# # # MiniLM-L6-L12-paraphrase \n",
    "# start = time.time()\n",
    "# global df_name\n",
    "# df_name = 'valid'\n",
    "\n",
    "# params_list = list(itertools.product(['minilmL12', 'minilm', 'paraphrase'], [0,1], [valid]))\n",
    "# with multiprocessing.Pool(len(params_list)) as pool:\n",
    "#     pool.map(encode_questions_minilm, params_list) \n",
    "    \n",
    "# with multiprocessing.Pool(3) as pool:\n",
    "#     pool.map(combine_embs_from_processes, [('minilmL12'), ('minilm'), ('paraphrase')])\n",
    "    \n",
    "# print(f'MiniLM-L6-L12-paraphrase-v2 encoding: {time.time() - start} s')\n",
    "# del df_name"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "01ee4660-69a9-431b-b647-4f3a3a473c11",
   "metadata": {},
   "source": [
    "### train "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "66238e11-8402-48f4-bdb1-6885ad7679b7",
   "metadata": {
    "scrolled": true,
    "tags": []
   },
   "outputs": [],
   "source": [
    "# # # MiniLM-L6-L12-paraphrase \n",
    "# start = time.time()\n",
    "# global df_name\n",
    "# df_name = 'train'\n",
    "\n",
    "# params_list = list(itertools.product(['minilmL12', 'minilm', 'paraphrase'], [0,1], [train]))\n",
    "# with multiprocessing.Pool(len(params_list)) as pool:\n",
    "#     pool.map(encode_questions_minilm, params_list) \n",
    "    \n",
    "# with multiprocessing.Pool(3) as pool:\n",
    "#     pool.map(combine_embs_from_processes, [('minilmL12'), ('minilm'), ('paraphrase')])\n",
    "    \n",
    "# print(f'MiniLM-L6-L12-paraphrase-v2 encoding: {time.time() - start} s')\n",
    "# del df_name"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e7c5fe41-073a-44e2-9b05-5231f8c2ef31",
   "metadata": {},
   "source": [
    "## bge + gte "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0050fb91-2dc4-4d0e-a742-0d38a381f72f",
   "metadata": {},
   "source": [
    "### encode question "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "060e6eb7-cb1e-44b8-851e-b64f34b0acac",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "def encode_questions_bge(args):\n",
    "    \"\"\"Using BAAI/bge-small-en-v1.5 model,\n",
    "    Generate embeddings for concatenation of prompt and answer options.\n",
    "    It is supposed to run in 2 processes, each embeds half of the test dataframe.\n",
    "    Result as float16 numpy array [len(df) x 384] is saved in pkl file.\n",
    "\n",
    "    Args:\n",
    "        proc_id (int): number of process we are in.\n",
    "    \"\"\"\n",
    "    proc_id, df = args\n",
    "    \n",
    "    if bge_name == 'bge': \n",
    "        model_path = '/mntdata/wangql43/A000Files/A003Model/AI-ModelScope/bge-large-en-v1.5/'\n",
    "    elif bge_name == 'gte': \n",
    "        model_path = '/mntdata/wangql43/A000Files/A003Model/iic/nlp_gte_sentence-embedding_english-large/'\n",
    "    elif bge_name == 'bgeM3':\n",
    "        model_path = '/mntdata/wangql43/A000Files/A003Model/AI-ModelScope/bge-m3/'\n",
    "    elif bge_name == 'bgeLarge':\n",
    "        model_path = '/mntdata/wangql43/A000Files/A003Model/AI-ModelScope/bge-reranker-large/'\n",
    "        # model_path = 'models/mixed_model_1/'\n",
    "        # model_path = '/data/wangql43/A000Files/A000comp/006kdd/AQA-KDD-2024/RAG-Retrieval/rag_retrieval/train/embedding/output/test_trainingModel/model/'\n",
    "        \n",
    "        \n",
    "    tokenizer = AutoTokenizer.from_pretrained(model_path)\n",
    "    model = AutoModel.from_pretrained(model_path)\n",
    "    model.eval()\n",
    "    model.to(f'cuda:{proc_id}')\n",
    "\n",
    "    # valpath = 'data/AQA/qa_valid_wo_ans.txt'\n",
    "    # df = read_train_valid_test(valpath)\n",
    "    # df['body'] = df['body'].apply(clean_body_remove_symbol)\n",
    "\n",
    "    df = np.array_split(df, 2)[proc_id]\n",
    "    texts = []\n",
    "    instr = \"Represent this sentence for searching relevant passages: \"\n",
    "    for _, row in df.iterrows():\n",
    "        colon = ':'\n",
    "        if row.question.endswith(':'):\n",
    "            colon = ''\n",
    "        text = f\"{instr}{row.question}{colon} {row.body}\"\n",
    "        texts.append(text)\n",
    "\n",
    "    embeddings = []\n",
    "    dataloader = DataLoader(\n",
    "        texts, batch_size=32, num_workers=0,\n",
    "        collate_fn=lambda batch: tokenizer(batch, max_length=512, padding=True, truncation=True, return_tensors='pt')\n",
    "    )\n",
    "    with torch.no_grad():\n",
    "        with torch.autocast(device_type='cuda'):\n",
    "            for batch in tqdm(dataloader):\n",
    "                model_output = model(**batch.to(model.device))\n",
    "                sentence_embeddings = model_output[0][:, 0]\n",
    "                sentence_embeddings = torch.nn.functional.normalize(sentence_embeddings, p=2, dim=1)\n",
    "                embeddings.append(sentence_embeddings)\n",
    "    embeddings = torch.cat(embeddings, dim=0).cpu().numpy().astype(np.float16)\n",
    "\n",
    "    with open(f'outslgb/encoded_questions_{bge_name}_{df_name}_{proc_id}.pkl', 'wb') as f:\n",
    "        pickle.dump(embeddings, f) \n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "31d4040d-42ef-4ddf-ba82-29595e2e3338",
   "metadata": {},
   "source": [
    "### test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "a8dcf7cd-f36c-4a0d-a4d0-d67df055672c",
   "metadata": {
    "scrolled": true,
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[None, None]"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "text/plain": [
       "[None, None]"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "text/plain": [
       "[None, None]"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "text/plain": [
       "[None, None, None]"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "bge-gte encoding: 49.956908226013184 s\n"
     ]
    }
   ],
   "source": [
    "## bge-gte\n",
    "start = time.time()\n",
    "\n",
    "global bge_name, df_name\n",
    "df_name = 'test'\n",
    "params_list = list(itertools.product([0,1], [test]))\n",
    "\n",
    "bge_name = 'bge'\n",
    "with multiprocessing.Pool(2) as pool:\n",
    "    pool.map(encode_questions_bge, params_list) \n",
    "    \n",
    "bge_name = 'gte'\n",
    "with multiprocessing.Pool(2) as pool:\n",
    "    pool.map(encode_questions_bge, params_list) \n",
    "    \n",
    "bge_name = 'bgeM3'\n",
    "with multiprocessing.Pool(2) as pool:\n",
    "    pool.map(encode_questions_bge, params_list) \n",
    "\n",
    "del bge_name; clean_memory()\n",
    "\n",
    "with multiprocessing.Pool(1) as pool:\n",
    "    pool.map(combine_embs_from_processes, ['bge', 'gte', 'bgeM3'])\n",
    "\n",
    "print(f'bge-gte encoding: {time.time() - start} s') \n",
    "\n",
    "del df_name"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0de36548-862f-4cd1-83ad-be0ef0667ebf",
   "metadata": {},
   "source": [
    "### valid"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "a36fa514-ce84-4493-bc32-4b5e9b9c7f07",
   "metadata": {
    "scrolled": true,
    "tags": []
   },
   "outputs": [],
   "source": [
    "# # bge-gte\n",
    "# start = time.time()\n",
    "\n",
    "# global bge_name, df_name\n",
    "# df_name = 'valid'\n",
    "# params_list = list(itertools.product([0,1], [valid]))\n",
    "\n",
    "# bge_name = 'bge'\n",
    "# with multiprocessing.Pool(2) as pool:\n",
    "#     pool.map(encode_questions_bge, params_list) \n",
    "    \n",
    "# bge_name = 'gte'\n",
    "# with multiprocessing.Pool(2) as pool:\n",
    "#     pool.map(encode_questions_bge, params_list) \n",
    "    \n",
    "# del bge_name; clean_memory()\n",
    "\n",
    "# with multiprocessing.Pool(1) as pool:\n",
    "#     pool.map(combine_embs_from_processes, ['bge', 'gte'])\n",
    "\n",
    "# print(f'bge-gte encoding: {time.time() - start} s') \n",
    "\n",
    "# del df_name"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0e7f86e7-0a5b-45d6-a68c-2372f9624e11",
   "metadata": {},
   "source": [
    "### train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "2ed25e1a-57b6-4ad9-b218-c5061f5eaf41",
   "metadata": {
    "scrolled": true,
    "tags": []
   },
   "outputs": [],
   "source": [
    "# # bge-gte\n",
    "# start = time.time()\n",
    "\n",
    "# global bge_name, df_name\n",
    "# df_name = 'train'\n",
    "# params_list = list(itertools.product([0,1], [train]))\n",
    "\n",
    "# bge_name = 'bge'\n",
    "# with multiprocessing.Pool(2) as pool:\n",
    "#     pool.map(encode_questions_bge, params_list) \n",
    "    \n",
    "# bge_name = 'gte'\n",
    "# with multiprocessing.Pool(2) as pool:\n",
    "#     pool.map(encode_questions_bge, params_list) \n",
    "    \n",
    "# del bge_name; clean_memory()\n",
    "\n",
    "# with multiprocessing.Pool(1) as pool:\n",
    "#     pool.map(combine_embs_from_processes, ['bge', 'gte'])\n",
    "\n",
    "# print(f'bge-gte encoding: {time.time() - start} s') \n",
    "\n",
    "# del df_name"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "14cbd0ba-0788-4651-8ca0-60dd26ce831a",
   "metadata": {},
   "source": [
    "## passage-embedding "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "7003e8e8-8a33-4554-9a67-d8e06f60025e",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "### minilm embedding \n",
    "def encode_passageJson_minilm(args):\n",
    "    \"\"\"Using all-MiniLM-L6-v2 model,\n",
    "    Generate embeddings for concatenation of prompt and answer options.\n",
    "    It is supposed to run in 2 processes, each embeds half of the test dataframe.\n",
    "    Result as float16 numpy array [len(df) x 384] is saved in pkl file.\n",
    "\n",
    "    Args:\n",
    "        proc_id (int): number of process we are in.\n",
    "    \"\"\"\n",
    "    minilm_name, proc_id, df = args\n",
    "    \n",
    "    if minilm_name == 'minilmL12': \n",
    "        model_name = '/mntdata/wangql43/A000Files/A003Model/bensonpeng/all-MiniLM-L12-v2/'\n",
    "    elif minilm_name == 'minilm': \n",
    "        model_name = '/mntdata/wangql43/A000Files/A003Model/bensonpeng/all-MiniLM-L6-v2/'\n",
    "    elif minilm_name == 'paraphrase': \n",
    "        model_name = '/mntdata/wangql43/A000Files/A003Model/bensonpeng/paraphrase-MiniLM-L12-v2/'\n",
    "        \n",
    "    model = SentenceTransformer(model_name, trust_remote_code=True)\n",
    "    \n",
    "    ## encode \n",
    "    df = np.array_split(df, 2)[proc_id]\n",
    "    texts = []\n",
    "    for _, row in df.iterrows():\n",
    "        text = f\"{row.title}\\n{row.abstract}\"\n",
    "        texts.append(text)\n",
    "    embs = model.encode(texts, device=f'cuda:{proc_id}', batch_size=256).astype(np.float16)\n",
    "    with open(f'outslgb/encoded_passageJson_{minilm_name}_{proc_id}.pkl', 'wb') as f:\n",
    "        pickle.dump(embs, f)\n",
    "        \n",
    "\n",
    "### bge embedding \n",
    "def encode_passageJson_bge(args):\n",
    "    \"\"\"Using BAAI/bge-small-en-v1.5 model,\n",
    "    Generate embeddings for concatenation of prompt and answer options.\n",
    "    It is supposed to run in 2 processes, each embeds half of the test dataframe.\n",
    "    Result as float16 numpy array [len(df) x 384] is saved in pkl file.\n",
    "\n",
    "    Args:\n",
    "        proc_id (int): number of process we are in.\n",
    "    \"\"\"\n",
    "    proc_id, df = args\n",
    "    \n",
    "    if bge_name == 'bge': \n",
    "        model_path = '/mntdata/wangql43/A000Files/A003Model/AI-ModelScope/bge-large-en-v1.5/'\n",
    "    elif bge_name == 'gte': \n",
    "        model_path = '/mntdata/wangql43/A000Files/A003Model/iic/nlp_gte_sentence-embedding_english-large/'\n",
    "    elif bge_name == 'bgeM3':\n",
    "        model_path = '/mntdata/wangql43/A000Files/A003Model/AI-ModelScope/bge-m3/'\n",
    "    elif bge_name == 'bgeLarge':\n",
    "        model_path = '/mntdata/wangql43/A000Files/A003Model/AI-ModelScope/bge-reranker-large/'\n",
    "        # model_path = 'models/mixed_model_1/'\n",
    "        # model_path = '/data/wangql43/A000Files/A000comp/006kdd/AQA-KDD-2024/RAG-Retrieval/rag_retrieval/train/embedding/output/test_trainingModel/model/'\n",
    "    \n",
    "    tokenizer = AutoTokenizer.from_pretrained(model_path)\n",
    "    model = AutoModel.from_pretrained(model_path, trust_remote_code=True) \n",
    "    model.eval()\n",
    "    model.to(f'cuda:{proc_id}')\n",
    "    \n",
    "    ## 读取 json 文件\n",
    "    df = np.array_split(df, 2)[proc_id]\n",
    "    texts = []\n",
    "    instr = \"Represent this sentence for searching relevant passages: \"\n",
    "    for _, row in df.iterrows():\n",
    "        colon = ':'\n",
    "        if row.title.endswith(':'):\n",
    "            colon = ''\n",
    "        text = f\"{instr}{row.title}{colon} {row.abstract}\"\n",
    "        texts.append(text)\n",
    "\n",
    "    embeddings = []\n",
    "    dataloader = DataLoader(\n",
    "        texts, batch_size=128, num_workers=0,\n",
    "        collate_fn=lambda batch: tokenizer(batch, max_length=512, padding=True, truncation=True, return_tensors='pt')\n",
    "    )\n",
    "    with torch.no_grad():\n",
    "        with torch.autocast(device_type='cuda'):\n",
    "            for batch in tqdm(dataloader):\n",
    "                model_output = model(**batch.to(model.device))\n",
    "                sentence_embeddings = model_output[0][:, 0]\n",
    "                sentence_embeddings = torch.nn.functional.normalize(sentence_embeddings, p=2, dim=1)\n",
    "                embeddings.append(sentence_embeddings)\n",
    "    embeddings = torch.cat(embeddings, dim=0).cpu().numpy().astype(np.float16)\n",
    "\n",
    "    with open(f'outslgb/encoded_passageJson_{bge_name}_{proc_id}.pkl', 'wb') as f:\n",
    "        pickle.dump(embeddings, f)\n",
    "\n",
    "### \n",
    "def combine_passageJson_embs_from_processes(model_name: str):\n",
    "    \"\"\"Question encoding functions produce two embeddings files for each model,\n",
    "    this one combines two into one. \n",
    "\n",
    "    Args:\n",
    "        model_name (str): 'bge' or 'minilm'.\n",
    "    \"\"\"\n",
    "    embs = []\n",
    "    for proc_idx in range(2):\n",
    "        with open(f'outslgb/encoded_passageJson_{model_name}_{proc_idx}.pkl', 'rb') as f:\n",
    "            embs.append(pickle.load(f))\n",
    "    embs = np.concatenate(embs, axis=0)\n",
    "    with open(f'outslgb/encoded_passageJson_{model_name}.pkl', 'wb') as f:\n",
    "        pickle.dump(embs, f) "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "31be153c-76c5-4e8e-83df-2ca75c5d1502",
   "metadata": {},
   "source": [
    "### passage - minilm "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "63715795-924b-477e-b135-ab9752798300",
   "metadata": {
    "scrolled": true,
    "tags": []
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of the model checkpoint at /mntdata/wangql43/A000Files/A003Model/bensonpeng/all-MiniLM-L6-v2/ were not used when initializing BertModel: ['classifier.bias', 'classifier.weight']\n",
      "- This IS expected if you are initializing BertModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing BertModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
      "Some weights of the model checkpoint at /mntdata/wangql43/A000Files/A003Model/bensonpeng/all-MiniLM-L6-v2/ were not used when initializing BertModel: ['classifier.bias', 'classifier.weight']\n",
      "- This IS expected if you are initializing BertModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing BertModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[None, None, None, None, None, None]"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "text/plain": [
       "[None, None, None]"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "MiniLM-L6-L12-paraphrase-v2 encoding: 719.149254322052 s\n"
     ]
    }
   ],
   "source": [
    "start = time.time()\n",
    "params_list = list(itertools.product(['minilmL12', 'minilm', 'paraphrase'], [0,1], [df_json]))\n",
    "with multiprocessing.Pool(len(params_list)) as pool:\n",
    "    pool.map(encode_passageJson_minilm, params_list) \n",
    "\n",
    "with multiprocessing.Pool(3) as pool:\n",
    "    pool.map(combine_passageJson_embs_from_processes, [('minilmL12'), ('minilm'), ('paraphrase')])\n",
    "print(f'MiniLM-L6-L12-paraphrase-v2 encoding: {time.time() - start} s')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "09423d05-7ba2-40e9-a297-9faa83d42e81",
   "metadata": {},
   "source": [
    "### passage - gte + bge "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "a664fe34-b941-4d6d-a6bd-e21c58e60b8c",
   "metadata": {
    "scrolled": true,
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[None, None]"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "text/plain": [
       "[None, None]"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "text/plain": [
       "[None, None]"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "text/plain": [
       "[None, None, None]"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "bge-large encoding: 4632.649368047714 s\n"
     ]
    }
   ],
   "source": [
    "start = time.time()\n",
    "## \n",
    "params_list = list(itertools.product([0,1], [df_json]))\n",
    "\n",
    "global bge_name\n",
    "bge_name = 'bge'\n",
    "with multiprocessing.Pool(2) as pool:\n",
    "    pool.map(encode_passageJson_bge, params_list) \n",
    "    \n",
    "bge_name = 'gte'\n",
    "with multiprocessing.Pool(2) as pool:\n",
    "    pool.map(encode_passageJson_bge, params_list) \n",
    "    \n",
    "bge_name = 'bgeM3'\n",
    "with multiprocessing.Pool(2) as pool:\n",
    "    pool.map(encode_passageJson_bge, params_list) \n",
    "    \n",
    "del bge_name; clean_memory()\n",
    "\n",
    "with multiprocessing.Pool(1) as pool:\n",
    "    pool.map(combine_passageJson_embs_from_processes, ['bge', 'gte', 'bgeM3'])\n",
    "\n",
    "# with multiprocessing.Pool(1) as pool:\n",
    "#     pool.map(combine_passageJson_embs_from_processes, ['gte'])\n",
    "    \n",
    "print(f'bge-large encoding: {time.time() - start} s') "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "54f6eb03-1c83-44c8-9806-219cc4db9643",
   "metadata": {},
   "source": [
    "### passage idx "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "08b2e5fd-97a2-4bea-a117-5edd95753232",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "## 给passages信息加上 id的映射 \n",
    "# df_json['id'] = df_json.index\n",
    "\n",
    "## id2pids\n",
    "id2pids_dict = dict(zip(df_json['id'], df_json['pids']))\n",
    "with open(f'outslgb/id2pids_dict.pkl', 'wb') as f:\n",
    "    pickle.dump(id2pids_dict, f) \n",
    "\n",
    "## pids2id\n",
    "pids2id_dict = dict(zip(df_json['pids'], df_json['id']))\n",
    "with open(f'outslgb/pids2id_dict.pkl', 'wb') as f:\n",
    "    pickle.dump(pids2id_dict, f) "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "15f60ab8-22bf-40fc-9126-73c350a4e272",
   "metadata": {},
   "source": [
    "## 每个 model 都来一对 query2passage topK passages "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "02cde554-b3af-4e6a-9e9a-7a302a0be6b8",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "def calculate_similarities(\n",
    "    model_name: str,\n",
    "    device: Union[str, int], \n",
    "    df_name: str\n",
    ") -> torch.Tensor:\n",
    "    '''\n",
    "    minilm_similarities = calculate_similarities(model_name='minilm', device='cuda:0', train)\n",
    "    '''\n",
    "    \n",
    "    ## 读取 query 的 embedding 内容 \n",
    "    with open(f'outslgb/encoded_questions_{model_name}_{df_name}.pkl', 'rb') as f:\n",
    "        query_embs = torch.tensor(pickle.load(f), dtype=torch.float16, device=device)\n",
    "        \n",
    "    ## passage 的 embedding 内容 \n",
    "    passages_embs_dir = {\n",
    "        ## sentence model \n",
    "        'minilm': 'outslgb/encoded_passageJson_minilm.pkl',\n",
    "        'minilmL12': 'outslgb/encoded_passageJson_minilmL12.pkl',\n",
    "        'paraphrase': 'outslgb/encoded_passageJson_paraphrase.pkl',\n",
    "        ## model \n",
    "        'bge': 'outslgb/encoded_passageJson_bge.pkl',\n",
    "        'gte': 'outslgb/encoded_passageJson_gte.pkl',\n",
    "        'bgeM3': 'outslgb/encoded_passageJson_bgeM3.pkl',\n",
    "        # 'bgeLarge': 'outslgb/encoded_passageJson_bgeLarge.pkl',\n",
    "    }[model_name]\n",
    "    with open(passages_embs_dir, 'rb') as f:\n",
    "        passages_embs = torch.tensor(pickle.load(f), dtype=torch.float16, device=device)\n",
    "        \n",
    "    clean_memory()\n",
    "    \n",
    "    ## 直接计算两者的相似度 \n",
    "    similarity = query_embs @ passages_embs.T\n",
    "    return similarity\n",
    "\n",
    "\n",
    "### \n",
    "def get_query_passages_similarities(df_name):\n",
    "    # Calculate similarities\n",
    "    args = [('minilm', 'cuda:0', df_name), \n",
    "            ('bge', 'cuda:1', df_name), \n",
    "            ('minilmL12', 'cuda:0', df_name), \n",
    "            ('gte', 'cuda:1', df_name)\n",
    "           ]\n",
    "    with ThreadPoolExecutor() as executor:\n",
    "        minilm_similarities, bge_similarities, minilm_similarities_l12, gte_similarities = tuple(executor.map(lambda p: calculate_similarities(*p), args))\n",
    "    clean_memory()\n",
    "    \n",
    "    return minilm_similarities, bge_similarities, minilm_similarities_l12, gte_similarities\n",
    "\n",
    "\n",
    "### \n",
    "def get_query_passages_similarities_addOther(df_name):\n",
    "    # Calculate similarities\n",
    "    args = [\n",
    "            ('paraphrase', 'cuda:0', df_name), \n",
    "            ('bgeM3', 'cuda:1', df_name), \n",
    "           ]\n",
    "    with ThreadPoolExecutor() as executor:\n",
    "        paraphrase_similarities, bgeM3_similarities = tuple(executor.map(lambda p: calculate_similarities(*p), args))\n",
    "    clean_memory()\n",
    "    \n",
    "    return paraphrase_similarities, bgeM3_similarities"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "7365cfb5-d20c-44dd-b8f9-b6f9a15ba78d",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "def get_top_passages_ids(minilm_similarities):\n",
    "    ### \n",
    "    n_queries = minilm_similarities.shape[0]\n",
    "    batch_size = 100\n",
    "    n_batches = math.ceil(n_queries/batch_size)\n",
    "\n",
    "    ## \n",
    "    top_indices, top_similarities = [], []\n",
    "    for i in range(n_batches):\n",
    "        start = i*batch_size\n",
    "        end = (i+1)*batch_size\n",
    "        b_top_indices = torch.argsort(minilm_similarities[start:end], dim=1, descending=True)\n",
    "        b_top_similarities = torch.take_along_dim(minilm_similarities[start:end], b_top_indices, dim=1)[:, :N_PASSAGES].cpu().numpy()\n",
    "        b_top_indices = b_top_indices[:, :N_PASSAGES].cpu().numpy()\n",
    "        top_indices.append(b_top_indices)\n",
    "        top_similarities.append(b_top_similarities)\n",
    "    top_similarities = np.concatenate(top_similarities, axis=0)\n",
    "    top_indices = np.concatenate(top_indices, axis=0)\n",
    "    \n",
    "    ## 输出 相似度分数 和 top_indices \n",
    "    return top_similarities, top_indices\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "55eeb63a-0bf8-4111-a7bf-05e786bf541f",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "def save_similar_indices(train_valid, top_similarities, top_indices, model_name): \n",
    "    ## id2pids_dict 映射表 \n",
    "    with open('outslgb/id2pids_dict.pkl', 'rb') as f:\n",
    "        id2pids_dict = pickle.load(f) \n",
    "        \n",
    "    ## top-indies \n",
    "    top_indices_pids = [[id2pids_dict[value] for value in sublist] for sublist in top_indices] \n",
    "    top_indices_pids = [','.join(subist) for subist in top_indices_pids]\n",
    "\n",
    "    ## top-similar\n",
    "    top_similarities_pids_score = [','.join([str(value) for value in subist]) for subist in top_similarities]\n",
    "    \n",
    "    ## 返回输出内容\n",
    "    train_valid[f'retrival_pids_{model_name}'] = top_indices_pids\n",
    "    train_valid[f'retrival_pids_scores_{model_name}'] = top_similarities_pids_score \n",
    "    \n",
    "    return train_valid"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "d618d2e7-3eeb-4183-a14a-42ede7ac6330",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# train_minilm_similarities, train_bge_similarities, train_minilm_similarities_l12, train_gte_similarities = get_query_passages_similarities('train')\n",
    "# valid_minilm_similarities, valid_bge_similarities, valid_minilm_similarities_l12, valid_gte_similarities = get_query_passages_similarities('valid')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "8101e8de-4ce5-4a5a-b55a-be6ce7d1a840",
   "metadata": {},
   "outputs": [],
   "source": [
    "test_minilm_similarities, test_bge_similarities, test_minilm_similarities_l12, test_gte_similarities = get_query_passages_similarities('test')\n",
    "test_paraphrase_similarities, test_bgeM3_similarities = get_query_passages_similarities_addOther('test')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "1bf172c9-f1c1-4edb-97d5-1cc1a972617c",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# N_PASSAGES = 300  ## 0.18366 llm-merged \n",
    "# N_PASSAGES = 200  ## 0.1833\n",
    "# N_PASSAGES = 100    ## 0.1836\n",
    "N_PASSAGES = 150  ## 0.18386 llm-merged \n",
    "\n",
    "\n",
    "def get_candidates_for_train_valid(train_valid, top_similarities, model_name='minilmL6', symbol='train'): \n",
    "    model_name_top_similarities, model_name_top_indices = get_top_passages_ids(top_similarities)\n",
    "    df_model_name_candidates = save_similar_indices(train_valid, model_name_top_similarities, model_name_top_indices, model_name)\n",
    "    df_model_name_candidates.to_parquet(f'outslgb/{symbol}_{model_name}_topK{N_PASSAGES}_candidates.parquet', index=False) \n",
    "    return df_model_name_candidates"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0fbf29af-955a-448a-a088-db98d3ee07a9",
   "metadata": {
    "tags": []
   },
   "source": [
    "### minilm-L6 "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "d20a5968-1298-43e8-9446-68e5fbd26e6f",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# train_minilmL6_candidates = get_candidates_for_train_valid(train, train_minilm_similarities, model_name='minilmL6', symbol='train')\n",
    "\n",
    "# valid_minilmL6_candidates = get_candidates_for_train_valid(valid, valid_minilm_similarities, model_name='minilmL6', symbol='valid')\n",
    "\n",
    "# del train_minilmL6_candidates, valid_minilmL6_candidates"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "d2419d8a-ea4e-4eb3-975d-a5a528450a71",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "test_minilmL6_candidates = get_candidates_for_train_valid(test, test_minilm_similarities, model_name='minilmL6', symbol='test')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ec855baa-c682-4975-9c26-b42db891c910",
   "metadata": {},
   "source": [
    "### minilm-L12 "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "b2c862e8-0b47-41c3-b442-c3cb5485e640",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# train_minilmL12_candidates = get_candidates_for_train_valid(train, train_minilm_similarities_l12, model_name='minilmL12', symbol='train')\n",
    "\n",
    "# valid_minilmL12_candidates = get_candidates_for_train_valid(valid, valid_minilm_similarities_l12, model_name='minilmL12', symbol='valid')\n",
    "\n",
    "# del train_minilmL12_candidates, valid_minilmL12_candidates"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "57705134-d3ba-4e3a-8e65-b2b523eae642",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "test_minilmL12_candidates = get_candidates_for_train_valid(test, test_minilm_similarities_l12, model_name='minilmL12', symbol='test')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "63a55a98-a60b-4a05-96ac-3eaba6cec3d3",
   "metadata": {
    "tags": []
   },
   "source": [
    "### bge "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "e8f820e5-b5b6-4cf8-9f7f-490e47ec90b2",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# train_bge_candidates = get_candidates_for_train_valid(train, train_bge_similarities, model_name='bge', symbol='train')\n",
    "\n",
    "# valid_bge_candidates = get_candidates_for_train_valid(valid, valid_bge_similarities, model_name='bge', symbol='valid')\n",
    "\n",
    "# del train_bge_candidates, valid_bge_candidates"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "b45fe58c-4229-408d-952f-3fe61fe5073e",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "test_bge_candidates = get_candidates_for_train_valid(test, test_bge_similarities, model_name='bge', symbol='test')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "40e6d089-2e9b-4b57-be9d-94afbb5d2856",
   "metadata": {},
   "source": [
    "### gte"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "7f1350e8-f16f-44a7-a0a5-7ed3ff2b974b",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# train_gte_candidates = get_candidates_for_train_valid(train, train_gte_similarities, model_name='gte', symbol='train')\n",
    "\n",
    "# valid_gte_candidates = get_candidates_for_train_valid(valid, valid_gte_similarities, model_name='gte', symbol='valid')\n",
    "\n",
    "# del train_gte_candidates, valid_gte_candidates"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "f8a20c9b-8331-46fc-89ad-08f4041e7255",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "test_gte_candidates = get_candidates_for_train_valid(test, test_gte_similarities, model_name='gte', symbol='test')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5892842b-41aa-4f6b-a3f6-697fb6dc4ca0",
   "metadata": {},
   "source": [
    "### paraphrase"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "1ab115e9-891b-443b-91af-053807175cce",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "test_paraphrase_candidates = get_candidates_for_train_valid(test, test_paraphrase_similarities, model_name='paraphrase', symbol='test')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d6f66e3b-381a-4b54-8d0b-0188b6343c0b",
   "metadata": {},
   "source": [
    "### bgeM3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "4b9fd5e8-8072-4ff3-80f6-113ebe1b88d6",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "test_bgeM3_candidates = get_candidates_for_train_valid(test, test_bgeM3_similarities, model_name='bgeM3', symbol='test')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cce08a74-a735-4636-beab-529bd106a891",
   "metadata": {
    "tags": []
   },
   "source": [
    "### bgeLarge"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "3287c9ed-3e32-40ef-b81c-a4300ed082e5",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "### 不行 \n",
    "### test_bgeLarge_candidates = get_candidates_for_train_valid(test, test_bgeLarge_similarities, model_name='bgeLarge', symbol='test')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a354193b-2371-4e42-90c7-dba55b103836",
   "metadata": {},
   "source": [
    "### 清理内存占用"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "40080722-4759-4e4e-8293-ff1a187300ae",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>question</th>\n",
       "      <th>body</th>\n",
       "      <th>pids</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Why would it ever be possible for Java to be f...</td>\n",
       "      <td>Sometimes Java outperforms C++ in benchmarks. ...</td>\n",
       "      <td>[619bb02b1c45e57ce901d5f1]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Are stacks the only reasonable way to structur...</td>\n",
       "      <td>Most architectures I've seen rely on a call st...</td>\n",
       "      <td>[53e99876b7602d97020b053d]</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                            question  \\\n",
       "0  Why would it ever be possible for Java to be f...   \n",
       "1  Are stacks the only reasonable way to structur...   \n",
       "\n",
       "                                                body  \\\n",
       "0  Sometimes Java outperforms C++ in benchmarks. ...   \n",
       "1  Most architectures I've seen rely on a call st...   \n",
       "\n",
       "                         pids  \n",
       "0  [619bb02b1c45e57ce901d5f1]  \n",
       "1  [53e99876b7602d97020b053d]  "
      ]
     },
     "execution_count": 37,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train.head(2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "id": "48902562-7466-4015-ae41-78ebfa256336",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# del train_minilm_similarities, train_bge_similarities, train_minilm_similarities_l12, train_gte_similarities\n",
    "# del valid_minilm_similarities, valid_bge_similarities, valid_minilm_similarities_l12, valid_gte_similarities"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "id": "96abbe35-3b07-4830-abc6-e444bfeb78a2",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "clean_memory()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "id": "d79e1b55-88b9-4c2a-b42c-aa0791149b9e",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "150"
      ]
     },
     "execution_count": 40,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "N_PASSAGES\n",
    "test_bgeM3_candidates = pd.read_parquet(f'outslgb/test_bgeM3_topK{N_PASSAGES}_candidates.parquet')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "id": "fc864c08-ff5d-42bf-9419-1e4c8a78786f",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>question</th>\n",
       "      <th>body</th>\n",
       "      <th>retrival_pids_minilmL6</th>\n",
       "      <th>retrival_pids_scores_minilmL6</th>\n",
       "      <th>retrival_pids_minilmL12</th>\n",
       "      <th>retrival_pids_scores_minilmL12</th>\n",
       "      <th>retrival_pids_bge</th>\n",
       "      <th>retrival_pids_scores_bge</th>\n",
       "      <th>retrival_pids_gte</th>\n",
       "      <th>retrival_pids_scores_gte</th>\n",
       "      <th>retrival_pids_paraphrase</th>\n",
       "      <th>retrival_pids_scores_paraphrase</th>\n",
       "      <th>retrival_pids_bgeM3</th>\n",
       "      <th>retrival_pids_scores_bgeM3</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Cloud Computing - Suggesting customers migrate...</td>\n",
       "      <td>AN OVERBROAD PATENT ON suggesting customer mig...</td>\n",
       "      <td>5390ac1820f70186a0eb4898,558bf479e4b02b9f07a43...</td>\n",
       "      <td>0.5815,0.5796,0.5737,0.5728,0.558,0.5503,0.548...</td>\n",
       "      <td>53e9b381b7602d9703e63436,5390b1d220f70186a0ee2...</td>\n",
       "      <td>0.6074,0.589,0.581,0.567,0.5513,0.539,0.5244,0...</td>\n",
       "      <td>5390ac1820f70186a0eb4898,5390b1d220f70186a0ee2...</td>\n",
       "      <td>0.754,0.7295,0.7026,0.6987,0.6963,0.6963,0.692...</td>\n",
       "      <td>5390ac1820f70186a0eb4898,64d641fe3fda6d7f06226...</td>\n",
       "      <td>0.8975,0.892,0.8867,0.877,0.877,0.877,0.874,0....</td>\n",
       "      <td>5390ac1820f70186a0eb4898,53e99a4eb7602d97022b1...</td>\n",
       "      <td>7.133,7.03,6.906,6.906,6.785,6.625,6.54,6.406,...</td>\n",
       "      <td>53e9a797b7602d97030d0e95,5390b1d220f70186a0ee2...</td>\n",
       "      <td>0.7544,0.7266,0.7104,0.707,0.705,0.705,0.7026,...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Requesting prior art on Google machine learnin...</td>\n",
       "      <td>Google is attempting to patent well known conc...</td>\n",
       "      <td>53e9ac5bb7602d970362a4f8,53e99d1ab7602d97025cd...</td>\n",
       "      <td>0.5146,0.508,0.4763,0.4714,0.4663,0.4648,0.459...</td>\n",
       "      <td>53e9a26bb7602d9702b72b1d,598d17fb0cf2ddbbee17d...</td>\n",
       "      <td>0.4414,0.428,0.427,0.4187,0.4028,0.3992,0.3965...</td>\n",
       "      <td>53e9a26bb7602d9702b72b1d,5d9edc1947c8f76646032...</td>\n",
       "      <td>0.685,0.6826,0.6777,0.674,0.665,0.664,0.6626,0...</td>\n",
       "      <td>53e9a26bb7602d9702b72b1d,60c2be866750f85387880...</td>\n",
       "      <td>0.8735,0.8696,0.867,0.8657,0.863,0.8623,0.862,...</td>\n",
       "      <td>5e60d4a093d709897cce6ce6,5d9edc1947c8f76646032...</td>\n",
       "      <td>7.223,6.84,6.793,6.766,6.656,6.46,6.46,6.42,6....</td>\n",
       "      <td>5c7569e8f56def97982e75bf,645647a5d68f896efae34...</td>\n",
       "      <td>0.699,0.6943,0.694,0.689,0.688,0.6875,0.687,0....</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                            question  \\\n",
       "0  Cloud Computing - Suggesting customers migrate...   \n",
       "1  Requesting prior art on Google machine learnin...   \n",
       "\n",
       "                                                body  \\\n",
       "0  AN OVERBROAD PATENT ON suggesting customer mig...   \n",
       "1  Google is attempting to patent well known conc...   \n",
       "\n",
       "                              retrival_pids_minilmL6  \\\n",
       "0  5390ac1820f70186a0eb4898,558bf479e4b02b9f07a43...   \n",
       "1  53e9ac5bb7602d970362a4f8,53e99d1ab7602d97025cd...   \n",
       "\n",
       "                       retrival_pids_scores_minilmL6  \\\n",
       "0  0.5815,0.5796,0.5737,0.5728,0.558,0.5503,0.548...   \n",
       "1  0.5146,0.508,0.4763,0.4714,0.4663,0.4648,0.459...   \n",
       "\n",
       "                             retrival_pids_minilmL12  \\\n",
       "0  53e9b381b7602d9703e63436,5390b1d220f70186a0ee2...   \n",
       "1  53e9a26bb7602d9702b72b1d,598d17fb0cf2ddbbee17d...   \n",
       "\n",
       "                      retrival_pids_scores_minilmL12  \\\n",
       "0  0.6074,0.589,0.581,0.567,0.5513,0.539,0.5244,0...   \n",
       "1  0.4414,0.428,0.427,0.4187,0.4028,0.3992,0.3965...   \n",
       "\n",
       "                                   retrival_pids_bge  \\\n",
       "0  5390ac1820f70186a0eb4898,5390b1d220f70186a0ee2...   \n",
       "1  53e9a26bb7602d9702b72b1d,5d9edc1947c8f76646032...   \n",
       "\n",
       "                            retrival_pids_scores_bge  \\\n",
       "0  0.754,0.7295,0.7026,0.6987,0.6963,0.6963,0.692...   \n",
       "1  0.685,0.6826,0.6777,0.674,0.665,0.664,0.6626,0...   \n",
       "\n",
       "                                   retrival_pids_gte  \\\n",
       "0  5390ac1820f70186a0eb4898,64d641fe3fda6d7f06226...   \n",
       "1  53e9a26bb7602d9702b72b1d,60c2be866750f85387880...   \n",
       "\n",
       "                            retrival_pids_scores_gte  \\\n",
       "0  0.8975,0.892,0.8867,0.877,0.877,0.877,0.874,0....   \n",
       "1  0.8735,0.8696,0.867,0.8657,0.863,0.8623,0.862,...   \n",
       "\n",
       "                            retrival_pids_paraphrase  \\\n",
       "0  5390ac1820f70186a0eb4898,53e99a4eb7602d97022b1...   \n",
       "1  5e60d4a093d709897cce6ce6,5d9edc1947c8f76646032...   \n",
       "\n",
       "                     retrival_pids_scores_paraphrase  \\\n",
       "0  7.133,7.03,6.906,6.906,6.785,6.625,6.54,6.406,...   \n",
       "1  7.223,6.84,6.793,6.766,6.656,6.46,6.46,6.42,6....   \n",
       "\n",
       "                                 retrival_pids_bgeM3  \\\n",
       "0  53e9a797b7602d97030d0e95,5390b1d220f70186a0ee2...   \n",
       "1  5c7569e8f56def97982e75bf,645647a5d68f896efae34...   \n",
       "\n",
       "                          retrival_pids_scores_bgeM3  \n",
       "0  0.7544,0.7266,0.7104,0.707,0.705,0.705,0.7026,...  \n",
       "1  0.699,0.6943,0.694,0.689,0.688,0.6875,0.687,0....  "
      ]
     },
     "execution_count": 41,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "test_bgeM3_candidates.head(2)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "23d4c462-c03c-45fe-88d4-a629d3cc7b88",
   "metadata": {},
   "source": [
    "### 构建结果数据集 "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "id": "6224442a-83cc-4593-ab58-5ca118bf432e",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# test_bgeM3_candidates['pids2score_minilmL6_dict'] = test_bgeM3_candidates.apply(lambda row: dict(zip(row['retrival_pids_minilmL6'].split(','), row['retrival_pids_scores_minilmL6'].split(','))), axis=1)\n",
    "# # test_bgeM3_candidates['pids2score_minilmL12_dict'] = test_bgeM3_candidates.apply(lambda row: dict(zip(row['retrival_pids_minilmL12'].split(','), row['retrival_pids_scores_minilmL12'].split(','))), axis=1)\n",
    "# # test_bgeM3_candidates['pids2score_bge_dict'] = test_bgeM3_candidates.apply(lambda row: dict(zip(row['retrival_pids_bge'].split(','), row['retrival_pids_scores_bge'].split(','))), axis=1)\n",
    "# # test_bgeM3_candidates['pids2score_gte_dict'] = test_bgeM3_candidates.apply(lambda row: dict(zip(row['retrival_pids_gte'].split(','), row['retrival_pids_scores_gte'].split(','))), axis=1)\n",
    "# # test_bgeM3_candidates['pids2score_paraphrase_dict'] = test_bgeM3_candidates.apply(lambda row: dict(zip(row['retrival_pids_paraphrase'].split(','), row['retrival_pids_scores_paraphrase'].split(','))), axis=1)\n",
    "# # test_bgeM3_candidates['pids2score_bgeM3_dict'] = test_bgeM3_candidates.apply(lambda row: dict(zip(row['retrival_pids_bgeM3'].split(','), row['retrival_pids_scores_bgeM3'].split(','))), axis=1)\n",
    "\n",
    "# cols = [col for col in test_bgeM3_candidates.columns if ('score' not in col) and (col not in ['question', 'body'])]\n",
    "# for col in cols: \n",
    "#     test_bgeM3_candidates[col] = test_bgeM3_candidates[col].str.split(',')\n",
    "\n",
    "# test_minilmL6_candidates = test_bgeM3_candidates[['question', 'body', 'retrival_pids_minilmL6', 'pids2score_minilmL6_dict']].explode(column='retrival_pids_minilmL6')\n",
    "# # test_minilmL12_candidates = test_bgeM3_candidates[['question', 'body', 'retrival_pids_minilmL12', 'pids2score_minilmL12_dict']].explode(column='retrival_pids_minilmL12')\n",
    "# # test_bge_candidates = test_bgeM3_candidates[['question', 'body', 'retrival_pids_bge', 'pids2score_bge_dict']].explode(column='retrival_pids_bge')\n",
    "# # test_gte_candidates = test_bgeM3_candidates[['question', 'body', 'retrival_pids_gte', 'pids2score_gte_dict']].explode(column='retrival_pids_gte')\n",
    "# # test_paraphrase_candidates = test_bgeM3_candidates[['question', 'body', 'retrival_pids_paraphrase', 'pids2score_paraphrase_dict']].explode(column='retrival_pids_paraphrase')\n",
    "# # test_bgeM3_candidates_short = test_bgeM3_candidates[['question', 'body', 'retrival_pids_bgeM3', 'pids2score_bgeM3_dict']].explode(column='retrival_pids_bgeM3')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3bd3537a-ecd2-4da4-906a-2a732fb5417e",
   "metadata": {},
   "source": [
    "## ranker-candidates 构建"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "id": "f90b1e59-7d3b-4f98-a30b-12caff92b513",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "count    3000.000000\n",
       "mean      557.904667\n",
       "std        92.519938\n",
       "min       263.000000\n",
       "25%       494.000000\n",
       "50%       559.000000\n",
       "75%       621.000000\n",
       "max       832.000000\n",
       "Name: candidates, dtype: float64"
      ]
     },
     "execution_count": 43,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "## 先暂时忽略顺序 \n",
    "def construct_candidates(row): \n",
    "    return list(set((row['retrival_pids_minilmL6'] + ',' + \\\n",
    "                     row['retrival_pids_minilmL12'] + ',' + \\\n",
    "                     row['retrival_pids_bge'] + ',' + \\\n",
    "                     row['retrival_pids_gte'] + ',' + \\\n",
    "                     row['retrival_pids_paraphrase'] + ',' + \\\n",
    "                     row['retrival_pids_bgeM3']                     \n",
    "                    ).split(',')))\n",
    "\n",
    "# train['candidates'] = train.apply(construct_candidates, axis=1)\n",
    "# valid['candidates'] = valid.apply(construct_candidates, axis=1)\n",
    "test['candidates'] = test_bgeM3_candidates.apply(construct_candidates, axis=1) \n",
    "test['candidates'].apply(lambda x: len(x)).describe()\n",
    "test.to_parquet('outslgb/test_remove_pids_from_candidates.parquet', index=False) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "id": "8840ffd6-270d-4cde-9873-c641b83065d3",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "## 使用 set 求差值 （移除掉召回正确的选项）\n",
    "def remove_pids_from_candidates(row): \n",
    "    return list(set(row['candidates']).difference(set(row['pids'])))\n",
    "\n",
    "# train['neg_candidates'] = train.apply(remove_pids_from_candidates, axis=1)\n",
    "# if 'neg_candidates' in train.columns: \n",
    "#     print(1)\n",
    "\n",
    "# if 'neg_candidates' in train.columns: \n",
    "#     train.to_parquet('outslgb/train_remove_pids_from_candidates.parquet', index=False)\n",
    "#     valid.to_parquet('outslgb/valid_remove_pids_from_candidates.parquet', index=False)\n",
    "#     test.to_parquet('outslgb/test_remove_pids_from_candidates.parquet', index=False) \n",
    "\n",
    "# train = pd.read_parquet('outslgb/train_remove_pids_from_candidates.parquet')\n",
    "# valid = pd.read_parquet('outslgb/valid_remove_pids_from_candidates.parquet')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "id": "12ce0096-4837-45b3-b176-3824ca60ac50",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "test = pd.read_parquet('outslgb/test_remove_pids_from_candidates.parquet')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "id": "a4e3777b-07ea-4f8f-9587-793ceb8530bc",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# df_pos_train = train.explode('pids', ignore_index=False)[['question', 'body', 'pids']]\n",
    "# df_pos_train['label'] = int(1) \n",
    "# df_neg_train = train.explode('neg_candidates', ignore_index=False)[['question', 'body', 'neg_candidates']]\n",
    "# df_neg_train['label'] = int(0)\n",
    "# df_neg_train = df_neg_train.rename(columns={'neg_candidates': 'pids'})\n",
    "# df_train = pd.concat([df_pos_train, df_neg_train], ignore_index=False)\n",
    "# df_train = df_train.sort_index().reset_index()\n",
    "# # del df_pos_train, df_neg_train\n",
    "\n",
    "# df_valid = valid.rename(columns={'candidates': 'pids'})[['question', 'body', 'pids']].explode('pids', ignore_index=False)\n",
    "# df_valid = df_valid.sort_index().reset_index()\n",
    "# df_valid['label'] = int(-1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "id": "a1efc7b9-7047-4961-9e6f-62eace210701",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "df_test = test.rename(columns={'candidates': 'pids'})[['question', 'body', 'pids']].explode('pids', ignore_index=False)\n",
    "df_test = df_test.sort_index().reset_index()\n",
    "df_test['label'] = int(-1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "id": "690bbbe3-46e7-4b19-9701-c74e2ff0911b",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# df_json.head(1)\n",
    "# df_train = df_train.merge(df_json, how='left', on='pids')\n",
    "# df_valid = df_valid.merge(df_json, how='left', on='pids')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "id": "e7754ed2-16cc-40cc-a8f1-ed5c8af62b66",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "df_test = df_test.merge(df_json, how='left', on='pids')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b75b6491-311d-4f85-b8bb-4e0fa5421f1a",
   "metadata": {},
   "source": [
    "## lgb-reranker 构建特征工程 "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "id": "ce4122a8-6075-4f32-9565-d5aa3868b756",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "from rapidfuzz import fuzz\n",
    "import jellyfish  ## https://jamesturk.github.io/jellyfish/\n",
    "\n",
    "from multiprocessing import Pool, cpu_count\n",
    "from functools import partial"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7df54422-02e8-43c0-a2d0-a6ed2b756bef",
   "metadata": {
    "tags": []
   },
   "source": [
    "### 规则特征工程 "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "id": "8a4ca840-8b10-44c9-9e32-f3cb52281767",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "## 使用规则的方式计算距离 \n",
    "def parallel_apply(df, func, num_threads):\n",
    "    '''\n",
    "    # return parallel_apply(df, partial(calculate_features, alist=alist), num_threads)    \n",
    "    '''\n",
    "    df_split = np.array_split(df, num_threads)\n",
    "    pool = Pool(num_threads)\n",
    "    df = pd.concat(pool.map(func, df_split))\n",
    "    pool.close()\n",
    "    pool.join()\n",
    "    return df\n",
    "\n",
    "# 为了在多进程中使用tqdm，我们需要一个自定义的封装器\n",
    "def tqdm_parallel_map(func, iterable, **kwargs):\n",
    "    with Pool(cpu_count()) as pool:\n",
    "        return list(tqdm(pool.imap(func=func, iterable=iterable), total=len(iterable), **kwargs))\n",
    "\n",
    "def calculate_features(df, alist):\n",
    "    cols = ['question', 'body', 'title', 'abstract']\n",
    "\n",
    "    ## 字段列的长度 \n",
    "    # for col in cols: \n",
    "    #     df[f'{col}_length'] = df[col].str.split(' ').str.len()\n",
    "        \n",
    "    ## 去重之后，词级别的占比 \n",
    "    # for pair in alist: \n",
    "    #     # df[f'{pair[0]}_{pair[1]}_word_overlap'] = df.apply(lambda x: len(set(x[pair[0]].split(' ')) & set(x[pair[1]].split(' '))), axis=1)\n",
    "    #     df[f'{pair[0]}_{pair[1]}_word_overlap'] = list(map(lambda x, y: len(set(x.split(' ')) & set(y.split(' '))), df[pair[0]], df[pair[1]]))\n",
    "\n",
    "    ## fuzz 距离 \n",
    "    for pair in alist: \n",
    "        # df[f'{pair[0]}_{pair[1]}_fuzzRatio'] = df.apply(lambda x: fuzz.ratio(x[pair[0]], x[pair[1]]), axis=1)\n",
    "        df[f'{pair[0]}_{pair[1]}_fuzzRatio'] = [fuzz.ratio(x, y) for x, y in zip(df[pair[0]], df[pair[1]])]\n",
    "        \n",
    "    ## jellyfish 距离 \n",
    "    for pair in alist: \n",
    "        # df[f'{pair[0]}_{pair[1]}_jaro_similarity'] = df.apply(lambda x: jellyfish.jaro_similarity(x[pair[0]], x[pair[1]]), axis=1)\n",
    "        # df[f'{pair[0]}_{pair[1]}_damerau_levenshtein_distance'] = df.apply(lambda x: jellyfish.damerau_levenshtein_distance(x[pair[0]], x[pair[1]]), axis=1)\n",
    "        # df[f'{pair[0]}_{pair[1]}_hamming_distance'] = df.apply(lambda x: jellyfish.hamming_distance(x[pair[0]], x[pair[1]]), axis=1)\n",
    "        df[f'{pair[0]}_{pair[1]}_jaro_similarity'] = [jellyfish.jaro_similarity(x, y) for x, y in zip(df[pair[0]], df[pair[1]])]\n",
    "        df[f'{pair[0]}_{pair[1]}_damerau_levenshtein_distance'] = [jellyfish.damerau_levenshtein_distance(x, y) for x, y in zip(df[pair[0]], df[pair[1]])]\n",
    "        df[f'{pair[0]}_{pair[1]}_hamming_distance'] = [jellyfish.hamming_distance(x, y) for x, y in zip(df[pair[0]], df[pair[1]])]\n",
    "        \n",
    "    return df\n",
    "\n",
    "def construct_feature_engineer(df):\n",
    "    alist = list(itertools.product(['question', 'body'], ['title', 'abstract']))\n",
    "    num_threads = 24  # 使用与CPU核心数相同数量的线程\n",
    "    # 使用自定义的tqdm并行映射函数\n",
    "    dfs = tqdm_parallel_map(partial(calculate_features, alist=alist), np.array_split(df, num_threads))\n",
    "    return pd.concat(dfs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d82acd42-f556-4915-bb61-93f1406eaa6e",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# %%time \n",
    "# ## .head(100)\n",
    "# df_train = construct_feature_engineer(df_train) \n",
    "\n",
    "# %%time \n",
    "# ## .head(100)\n",
    "# df_valid = construct_feature_engineer(df_valid)\n",
    "\n",
    "# %%time \n",
    "# ## .head(100)\n",
    "# df_test = construct_feature_engineer(df_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "id": "56eda37f-06b5-44a1-bbc6-c19297df9f38",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# # 计算 DataFrame 占用的内存量（默认情况下包括了索引）\n",
    "# # 将字节转换为 MB\n",
    "# memory_usage_in_mb = df_train.memory_usage(deep=True).sum() / (1024 ** 2)\n",
    "# print(f\"df_train >>> DataFrame 占用的内存为: {memory_usage_in_mb:.2f} MB\")\n",
    "\n",
    "# # 计算 DataFrame 占用的内存量（默认情况下包括了索引）\n",
    "# # 将字节转换为 MB\n",
    "# memory_usage_in_mb = df_valid.memory_usage(deep=True).sum() / (1024 ** 2)\n",
    "# print(f\"df_valid >>> DataFrame 占用的内存为: {memory_usage_in_mb:.2f} MB\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "id": "9aec490a-0892-4028-8648-26c327fbcddf",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(1673714, 8)"
      ]
     },
     "execution_count": 57,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_test.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "103d125a-5149-43a8-a654-3e1307277d1c",
   "metadata": {},
   "source": [
    "### unique_index "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "id": "56d826b0-8bee-4ae0-ad0d-e3caff9af67e",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "unique_index = df_test['index'].unique()\n",
    "\n",
    "device = 'cuda:0'"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cbd0cabb-9311-4cde-a25c-7131ffb1c2e2",
   "metadata": {
    "tags": []
   },
   "source": [
    "### 各自 embeds 特征工程 @ 相似度 -- 直接通过前期的 embedding dict 进行计算 "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "id": "ac3932cb-cbaf-41f0-b19c-f8143623d2ac",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "model_name   >>>   minilm   ...   \n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "173cf49a8c9c4602883ee2b90c6e6807",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/3000 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "model_name   >>>   minilmL12   ...   \n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "064f71766f494c5793f544d72f4c18b2",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/3000 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "model_name   >>>   bge   ...   \n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "a6cf35a140074b46ae1b33bd53e0653b",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/3000 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "model_name   >>>   gte   ...   \n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "967310fa321a4885832643e9e8d1071c",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/3000 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "model_name   >>>   bgeM3   ...   \n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "a3ea3b3d86ef463883a8fc3181954352",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/3000 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "model_name_lst = ['minilm', 'minilmL12', 'bge', 'gte', 'bgeM3']\n",
    "\n",
    "for model_name in model_name_lst: \n",
    "    print(f'model_name   >>>   {model_name}   ...   ')\n",
    "    ## 读取 query 的 embedding 内容 \n",
    "    with open(f'outslgb/encoded_questions_{model_name}_test.pkl', 'rb') as f:\n",
    "        query_embs = torch.tensor(pickle.load(f), dtype=torch.float16, device=device)\n",
    "    \n",
    "    ## passage 的 embedding 内容 \n",
    "    passages_embs_dir = {\n",
    "        ## sentence model \n",
    "        'minilm': 'outslgb/encoded_passageJson_minilm.pkl',\n",
    "        'minilmL12': 'outslgb/encoded_passageJson_minilmL12.pkl',\n",
    "        ## 'paraphrase': 'outslgb/encoded_passageJson_paraphrase.pkl',\n",
    "        ## model \n",
    "        'bge': 'outslgb/encoded_passageJson_bge.pkl',\n",
    "        'gte': 'outslgb/encoded_passageJson_gte.pkl',\n",
    "        'bgeM3': 'outslgb/encoded_passageJson_bgeM3.pkl',\n",
    "    }[model_name]\n",
    "    with open(passages_embs_dir, 'rb') as f:\n",
    "        passages_embs = torch.tensor(pickle.load(f), dtype=torch.float16, device=device)\n",
    "    # passages_embs_pids2dict = dict(zip(df_json['pids'].tolist(), passages_embs)) \n",
    "    \n",
    "    ## 计算得分 \n",
    "    for index in tqdm(unique_index, total=len(unique_index)): \n",
    "        retrival_pids = df_test.loc[df_test['index'] == index, 'id'].tolist() \n",
    "        scores = query_embs[index] @ passages_embs[retrival_pids].T\n",
    "        df_test.loc[df_test['index'] == index, f'{model_name}_q2p_scores'] = scores.detach().cpu().numpy().astype(np.float32)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "id": "cff43c06-e64c-455f-ab82-28ebc16dac5c",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "q2p_scores_cols = [col for col in df_test.columns if 'q2p_scores' in col]\n",
    "q2p_scores_cols = ['minilm_q2p_scores',\n",
    "                'minilmL12_q2p_scores',\n",
    "                # 'paraphrase_q2p_scores',\n",
    "                'bge_q2p_scores',\n",
    "                'gte_q2p_scores',\n",
    "                'bgeM3_q2p_scores']\n",
    "df_test['rank_score_mean'] = df_test[q2p_scores_cols].fillna(0).mean(axis=1).astype(np.float32)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "id": "82591653-16c0-4496-818b-894be4261fbd",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1.0"
      ]
     },
     "execution_count": 61,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def get_rank_scores(row): \n",
    "    rank_score = 0.2 * row['minilm_q2p_scores'] + \\\n",
    "                0.2 * row['minilmL12_q2p_scores'] + \\\n",
    "                0.2 * row['bge_q2p_scores'] + \\\n",
    "                0.2 * row['gte_q2p_scores'] + \\\n",
    "                0.2 * row['bgeM3_q2p_scores']\n",
    "    return rank_score\n",
    "\n",
    "# 直接使用 numpy 进行向量化计算\n",
    "weights = np.array([0.18, 0.16, 0.24, 0.2, 0.22])  # 权重向量\n",
    "np.sum(weights)\n",
    "q2p_scores_cols = ['minilm_q2p_scores',\n",
    "                'minilmL12_q2p_scores',\n",
    "                'bge_q2p_scores',\n",
    "                'gte_q2p_scores',\n",
    "                'bgeM3_q2p_scores']\n",
    "# 确保数据类型正确，避免计算时出现类型错误\n",
    "df_test[q2p_scores_cols] = df_test[q2p_scores_cols].astype(float) \n",
    "# 计算加权得分\n",
    "df_test['rank_score_weights'] = np.sum(df_test[q2p_scores_cols].values * weights, axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "id": "0e26fa38-e2ad-44f5-b15b-2e28e7b9f55c",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "def standardize_data(X):\n",
    "    # 计算每一列的均值和标准差\n",
    "    mean = np.mean(X, axis=0)\n",
    "    std = np.std(X, axis=0)\n",
    "    \n",
    "    # 对数据进行标准化\n",
    "    standardized_X = (X - mean) / std\n",
    "    \n",
    "    return standardized_X\n",
    "\n",
    "def min_max_normalization(X):\n",
    "    # 确保数据类型为浮点数，以避免整数除法的问题\n",
    "    X = X.astype('float64')\n",
    "    \n",
    "    # 计算每一列（特征）的最大值和最小值\n",
    "    max_values = np.max(X, axis=0)\n",
    "    min_values = np.min(X, axis=0)\n",
    "    \n",
    "    # 执行最大最小值归一化\n",
    "    normalized_X = (X - min_values) / (max_values - min_values)\n",
    "    \n",
    "    return normalized_X\n",
    "# df_test.groupby(by=['index'])['paraphrase_q2p_scores'].transform(min_max_normalization)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "id": "f70ddc0d-b9db-45fa-8b7f-c72062b28ce2",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# ### 0.1484\n",
    "# df_test = df_test.sort_values(by=['index', 'rank_score'], ascending=[True, False])\n",
    "# test_candidates_list = df_test.groupby(by=['index'], as_index=True)['pids'].apply(list)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "id": "7bf4c728-18f6-4096-9dc2-a80288425e1d",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "### 0.1486\n",
    "df_test = df_test.sort_values(by=['index', 'rank_score_weights'], ascending=[True, False])\n",
    "test_candidates_list_weight = df_test.groupby(by=['index'], as_index=True)['pids'].apply(list)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "id": "42151d96-736e-444e-8c63-28f6e7fdd931",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "test_candidates_list = test_candidates_list_weight.copy() \n",
    "test_candidates_list = test_candidates_list.apply(lambda x: ','.join(x[:20]))\n",
    "\n",
    "test_candidates_list.to_csv('my_data.txt', sep='\\t', index=False, header=None)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6c84c6bd-b8f4-4b77-ac90-4eface5b8e1e",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "843ddf30-5464-4a33-b30e-fcfc3e841235",
   "metadata": {},
   "source": [
    "### concat embeds 计算得分 rank_score_concat "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "id": "adb9add1-0367-4c82-8fbd-571545c32662",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "model_name   >>>   minilm   ...   \n",
      "model_name   >>>   minilmL12   ...   \n",
      "model_name   >>>   bge   ...   \n",
      "model_name   >>>   gte   ...   \n",
      "model_name   >>>   bgeM3   ...   \n"
     ]
    }
   ],
   "source": [
    "query_embs_concat = []\n",
    "passages_embs_concat = [] \n",
    "\n",
    "model_name_lst = ['minilm', 'minilmL12', 'bge', 'gte', 'bgeM3']\n",
    "for model_name in model_name_lst: \n",
    "    print(f'model_name   >>>   {model_name}   ...   ')\n",
    "    ## 读取 query 的 embedding 内容 \n",
    "    with open(f'outslgb/encoded_questions_{model_name}_test.pkl', 'rb') as f:\n",
    "        query_embs = torch.tensor(pickle.load(f), dtype=torch.float16, device=device)\n",
    "    query_embs_concat.append(query_embs) \n",
    "    \n",
    "    ## passage 的 embedding 内容 \n",
    "    passages_embs_dir = {\n",
    "        ## sentence model \n",
    "        'minilm': 'outslgb/encoded_passageJson_minilm.pkl',\n",
    "        'minilmL12': 'outslgb/encoded_passageJson_minilmL12.pkl',\n",
    "        ## 'paraphrase': 'outslgb/encoded_passageJson_paraphrase.pkl',\n",
    "        ## model \n",
    "        'bge': 'outslgb/encoded_passageJson_bge.pkl',\n",
    "        'gte': 'outslgb/encoded_passageJson_gte.pkl',\n",
    "        'bgeM3': 'outslgb/encoded_passageJson_bgeM3.pkl',\n",
    "    }[model_name]\n",
    "    with open(passages_embs_dir, 'rb') as f:\n",
    "        passages_embs = torch.tensor(pickle.load(f), dtype=torch.float16, device=device)\n",
    "    passages_embs_concat.append(passages_embs) \n",
    "\n",
    "query_embs_concat = torch.hstack(query_embs_concat) \n",
    "passages_embs_concat = torch.hstack(passages_embs_concat) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "id": "4576ccea-df63-4394-b6b9-7eb9ddeb8edb",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([3000, 3840])"
      ]
     },
     "execution_count": 67,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "text/plain": [
       "torch.Size([466387, 3840])"
      ]
     },
     "execution_count": 67,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "query_embs_concat.shape\n",
    "passages_embs_concat.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "id": "57459061-c112-498d-b852-5ad2f24a0d91",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "f0cdf869397448ef9566d50d11b0ecc7",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/3000 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "## 计算得分 \n",
    "model_name = 'concat'\n",
    "for index in tqdm(unique_index, total=len(unique_index)): \n",
    "    retrival_pids = df_test.loc[df_test['index'] == index, 'id'].tolist() \n",
    "    scores = query_embs_concat[index] @ passages_embs_concat[retrival_pids].T\n",
    "    df_test.loc[df_test['index'] == index, f'rank_score_{model_name}'] = scores.detach().cpu().numpy().astype(np.float32) \n",
    "    \n",
    "df_test['rank_score_concat'] = df_test.groupby(by=['index'])['rank_score_concat'].transform(min_max_normalization)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "id": "7c235f80-5c3c-4276-85d0-b7297c27ba2a",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "### 0.1482\n",
    "test_candidates_list = df_test.sort_values(by=['index', 'rank_score_concat'], \n",
    "                                           ascending=[True, False]).groupby(by=['index'], as_index=True)['pids'].apply(list)\n",
    "test_candidates_list = test_candidates_list.apply(lambda x: ','.join(x[:20]))\n",
    "test_candidates_list.to_csv('my_data_concat.txt', sep='\\t', index=False, header=None)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7af26096-89c0-43a0-bbaa-640b5df3ff80",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "0f471803-481f-4bc6-a31f-eac1a853b9a1",
   "metadata": {},
   "source": [
    "### 大模型 LLM 的计算得分 rank_score_llm_mistral"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "id": "53a7dad3-102d-4b93-9f24-6743d8ec8a82",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "model_name   >>>   Linq_mistral-NoClean   ...   \n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "418f647fa8964aa9930d431e3775c090",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/3000 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "model_name   >>>   srf_mistral-NoClean   ...   \n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "ba71b39398f042598d0c3e5b84d49c64",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/3000 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "model_name   >>>   Linq_mistral   ...   \n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "63c188a1b2bb44b19b1effd651f129d4",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/3000 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "model_name   >>>   srf_mistral   ...   \n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "163c53c479294f7db136d23467d0d9c5",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/3000 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "model_name   >>>   Linq_mistral-WithAiresponse   ...   \n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "c1c8f2cb25d14805a0217226d72c50bc",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/3000 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "model_name   >>>   srf_mistral-WithAiresponse   ...   \n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "50adf84cd09347dd8987d665e2bebc62",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/3000 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "model_name   >>>   Linq_mistral-WithKeywords   ...   \n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "5f29aee292884947b54aed8babff555e",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/3000 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "model_name   >>>   srf_mistral-WithKeywords   ...   \n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "327cdaac9230486aacef85add8f0a84c",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/3000 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "model_name   >>>   Linq_mistral-WithKeywords-glm4   ...   \n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "116a3bcdf367426dafd09428ef575d82",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/3000 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "model_name   >>>   srf_mistral-WithKeywords-glm4   ...   \n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "20ed7982614c433b8adf8621cae29a13",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/3000 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "for model_name in ['Linq_mistral-NoClean', 'srf_mistral-NoClean', \n",
    "                   'Linq_mistral', 'srf_mistral', \n",
    "                   'Linq_mistral-WithAiresponse', 'srf_mistral-WithAiresponse', \n",
    "                   'Linq_mistral-WithKeywords', 'srf_mistral-WithKeywords', \n",
    "                   'Linq_mistral-WithKeywords-glm4', 'srf_mistral-WithKeywords-glm4', \n",
    "                  ]: \n",
    "    print(f'model_name   >>>   {model_name}   ...   ') \n",
    "    ## query-embs \n",
    "    with open(f'outslgb/encoded_question_{model_name}_test.pkl', 'rb') as f:\n",
    "        query_embs = torch.tensor(pickle.load(f), dtype=torch.float16, device=device)\n",
    "    \n",
    "    ## passage-embs\n",
    "    passage_dir = f'outslgb/encoded_passageJson_{model_name}_2048.pkl'\n",
    "    if ('With' in passage_dir) or ('No' in passage_dir): \n",
    "        passage_dir = passage_dir.replace('-WithAiresponse', '').replace('-WithKeywords', '').replace('-NoClean', '').replace('-glm4', '')\n",
    "    with open(passage_dir, 'rb') as f:\n",
    "        passages_embs = torch.tensor(pickle.load(f), dtype=torch.float16, device=device)\n",
    "\n",
    "    ## 计算得分 \n",
    "    for index in tqdm(unique_index, total=len(unique_index)): \n",
    "        retrival_pids = df_test.loc[df_test['index'] == index, 'id'].tolist() \n",
    "        scores = query_embs[index] @ passages_embs[retrival_pids].T\n",
    "        df_test.loc[df_test['index'] == index, f'rank_score_{model_name}'] = scores.detach().cpu().numpy().astype(np.float32)\n",
    "    df_test[f'rank_score_{model_name}'] = df_test[f'rank_score_{model_name}'].astype(float) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "id": "731a9f1d-463e-45b1-ae14-48f221db7611",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "### srf_mistral  0.1820\n",
    "test_candidates_list = df_test.sort_values(by=['index', 'rank_score_srf_mistral'], \n",
    "                                           ascending=[True, False]).groupby(by=['index'], as_index=True)['pids'].apply(list)\n",
    "test_candidates_list = test_candidates_list.apply(lambda x: ','.join(x[:20]))\n",
    "test_candidates_list.to_csv('my_data_srf_mistral.txt', sep='\\t', index=False, header=None) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "id": "aaf722e1-8281-4ad5-b29d-7df519e49e2b",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "### Linq_mistral 0.1824\n",
    "test_candidates_list = df_test.sort_values(by=['index', 'rank_score_Linq_mistral'], \n",
    "                                           ascending=[True, False]).groupby(by=['index'], as_index=True)['pids'].apply(list)\n",
    "test_candidates_list = test_candidates_list.apply(lambda x: ','.join(x[:20]))\n",
    "test_candidates_list.to_csv('my_data_Linq_mistral.txt', sep='\\t', index=False, header=None) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "id": "0cc07faf-d138-4e7a-822a-55dd2cfd2b09",
   "metadata": {},
   "outputs": [],
   "source": [
    "### srf_mistral-NoClean \n",
    "test_candidates_list = df_test.sort_values(by=['index', 'rank_score_srf_mistral-NoClean'], \n",
    "                                           ascending=[True, False]).groupby(by=['index'], as_index=True)['pids'].apply(list)\n",
    "test_candidates_list = test_candidates_list.apply(lambda x: ','.join(x[:20]))\n",
    "test_candidates_list.to_csv('my_data_srf_mistral-NoClean.txt', sep='\\t', index=False, header=None) \n",
    "\n",
    "### Linq_mistral-NoClean \n",
    "test_candidates_list = df_test.sort_values(by=['index', 'rank_score_Linq_mistral-NoClean'], \n",
    "                                           ascending=[True, False]).groupby(by=['index'], as_index=True)['pids'].apply(list)\n",
    "test_candidates_list = test_candidates_list.apply(lambda x: ','.join(x[:20]))\n",
    "test_candidates_list.to_csv('my_data_Linq_mistral-NoClean.txt', sep='\\t', index=False, header=None) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "id": "0fe70c44-5f27-47e4-b63e-029392dff728",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "### srf_mistral-WithAiresponse  0.1777\n",
    "test_candidates_list = df_test.sort_values(by=['index', 'rank_score_srf_mistral-WithAiresponse'], \n",
    "                                           ascending=[True, False]).groupby(by=['index'], as_index=True)['pids'].apply(list)\n",
    "test_candidates_list = test_candidates_list.apply(lambda x: ','.join(x[:20]))\n",
    "test_candidates_list.to_csv('my_data_srf_mistral-WithAiresponse.txt', sep='\\t', index=False, header=None) \n",
    "\n",
    "### Linq_mistral-WithAiresponse  0.1771\n",
    "test_candidates_list = df_test.sort_values(by=['index', 'rank_score_Linq_mistral-WithAiresponse'], \n",
    "                                           ascending=[True, False]).groupby(by=['index'], as_index=True)['pids'].apply(list)\n",
    "test_candidates_list = test_candidates_list.apply(lambda x: ','.join(x[:20]))\n",
    "test_candidates_list.to_csv('my_data_Linq_mistral-WithAiresponse.txt', sep='\\t', index=False, header=None) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "id": "eca61cc0-5961-424e-837c-65f8872bcc92",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "### srf_mistral-WithKeywords 0.1755\n",
    "test_candidates_list = df_test.sort_values(by=['index', 'rank_score_srf_mistral-WithKeywords'], \n",
    "                                           ascending=[True, False]).groupby(by=['index'], as_index=True)['pids'].apply(list)\n",
    "test_candidates_list = test_candidates_list.apply(lambda x: ','.join(x[:20]))\n",
    "test_candidates_list.to_csv('my_data_srf_mistral-WithKeywords.txt', sep='\\t', index=False, header=None) \n",
    "\n",
    "### Linq_mistral-WithKeywords\n",
    "test_candidates_list = df_test.sort_values(by=['index', 'rank_score_Linq_mistral-WithKeywords'], \n",
    "                                           ascending=[True, False]).groupby(by=['index'], as_index=True)['pids'].apply(list)\n",
    "test_candidates_list = test_candidates_list.apply(lambda x: ','.join(x[:20]))\n",
    "test_candidates_list.to_csv('my_data_Linq_mistral-WithKeywords.txt', sep='\\t', index=False, header=None) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "id": "353667e1-ef2a-441c-b83c-453a84e356f1",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "### srf_mistral-WithKeywords 0.1776\n",
    "test_candidates_list = df_test.sort_values(by=['index', 'rank_score_srf_mistral-WithKeywords-glm4'], \n",
    "                                           ascending=[True, False]).groupby(by=['index'], as_index=True)['pids'].apply(list)\n",
    "test_candidates_list = test_candidates_list.apply(lambda x: ','.join(x[:20]))\n",
    "test_candidates_list.to_csv('my_data_srf_mistral-WithKeywords-glm4.txt', sep='\\t', index=False, header=None) \n",
    "\n",
    "### Linq_mistral-WithKeywords\n",
    "test_candidates_list = df_test.sort_values(by=['index', 'rank_score_Linq_mistral-WithKeywords-glm4'], \n",
    "                                           ascending=[True, False]).groupby(by=['index'], as_index=True)['pids'].apply(list)\n",
    "test_candidates_list = test_candidates_list.apply(lambda x: ','.join(x[:20]))\n",
    "test_candidates_list.to_csv('my_data_Linq_mistral-WithKeywords-glm4.txt', sep='\\t', index=False, header=None) "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b81e0284-e06a-46fe-9352-68a631aaf666",
   "metadata": {},
   "source": [
    "```python\n",
    "'rank_score_srf_mistral'\n",
    "'rank_score_Linq_mistral'\n",
    "\n",
    "'rank_score_Linq_mistral-NoClean'\n",
    "'rank_score_srf_mistral-NoClean', \n",
    "\n",
    "'rank_score_srf_mistral-WithAiresponse'\n",
    "'rank_score_Linq_mistral-WithAiresponse'\n",
    "'rank_score_srf_mistral-WithKeywords'\n",
    "'rank_score_Linq_mistral-WithKeywords'\n",
    "'rank_score_Linq_mistral-WithKeywords-glm4'\n",
    "'rank_score_Linq_mistral-WithKeywords-glm4'\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "id": "7c8d3220-42c0-4b31-b437-ecb27d2108db",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "##  0.1829 (ori + ai_response) \n",
    "##  0.1834 (0.52*ori + 0.28*ai_response + 0.2*keywords) \n",
    "##  0.1838 (0.5*ori + 0.3*ai_response + 0.2*ai_response) \n",
    "##  0.1824 (0.5*ori + 0.3*ai_response + 0.2*WithKeywords) \n",
    "# ##  0.1839\n",
    "df_test['rank_score_llm_merge'] = 0.5*(0.48*df_test['rank_score_srf_mistral'] + 0.52*df_test['rank_score_Linq_mistral']) + \\\n",
    "                0.5*(0.5*df_test['rank_score_srf_mistral-NoClean'] + 0.5*df_test['rank_score_Linq_mistral-NoClean']) + \\\n",
    "                0.5*(0.53*df_test['rank_score_srf_mistral-WithAiresponse'] + 0.47*df_test['rank_score_Linq_mistral-WithAiresponse']) + \\\n",
    "                0.5*(0.5*df_test['rank_score_srf_mistral-WithAiresponse'] + 0.5*df_test['rank_score_Linq_mistral-WithAiresponse']) \n",
    "\n",
    "\n",
    "# df_test['rank_score_llm_merge'] = 0.5*(0.48*df_test['rank_score_srf_mistral'] + 0.52*df_test['rank_score_Linq_mistral']) + \\\n",
    "#                 0.5*(0.5*df_test['rank_score_srf_mistral-WithAiresponse'] + 0.5*df_test['rank_score_Linq_mistral-WithAiresponse'])# + \\\n",
    "#                 # 0.2*(0.25*df_test['rank_score_srf_mistral-WithKeywords'] + 0.25*df_test['rank_score_Linq_mistral-WithKeywords'] + 0.28*df_test['rank_score_srf_mistral-WithKeywords-glm4'] + 0.28*df_test['rank_score_Linq_mistral-WithKeywords-glm4'])\n",
    "\n",
    "# ## \n",
    "# df_test['rank_score_llm_merge'] = 0.3*(0.48*df_test['rank_score_srf_mistral'] + 0.52*df_test['rank_score_Linq_mistral']) + \\\n",
    "#                 0.3*(0.5*df_test['rank_score_srf_mistral-NoClean'] + 0.5*df_test['rank_score_Linq_mistral-NoClean']) + \\\n",
    "#                 0.2*(0.5*df_test['rank_score_srf_mistral-WithAiresponse'] + 0.5*df_test['rank_score_Linq_mistral-WithAiresponse'])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "id": "0ddf2902-6748-48ef-acd6-19fdcdb29e42",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "### merged \n",
    "test_candidates_list = df_test.sort_values(by=['index', 'rank_score_llm_merge'], \n",
    "                                           ascending=[True, False]).groupby(by=['index'], as_index=True)['pids'].apply(list)\n",
    "test_candidates_list = test_candidates_list.apply(lambda x: ','.join(x[:20])) \n",
    "test_candidates_list.to_csv('my_data_llm_merge-noclean-airesponse.txt', sep='\\t', index=False, header=None) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2a028b46-51c9-4ff7-80b8-ca8c4ec7e6a1",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "087a8bb4-0f54-43bd-a34f-3f9c13598354",
   "metadata": {
    "tags": []
   },
   "source": [
    "## 融合分数 \n",
    "```\n",
    "得分 = 0.18006 : 0.18*concat + 0.4*srf-mistral + 0.42*linq-mistral\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "id": "c89443a3-b1ca-489f-a8cd-bc96db9c4363",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1.0"
      ]
     },
     "execution_count": 81,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# 直接使用 numpy 进行向量化计算\n",
    "weights = np.array([0.1, 0.15, 0.15, 0.1, 0.1, 0.1, 0.1, 0.1, 0.1]) \n",
    "np.sum(weights)\n",
    "q2p_scores_cols = [# 'rank_score_mean',            ### 0.14840\n",
    "                   # 'rank_score_weights',         ### 0.14827\n",
    "                    'rank_score_concat',          ### 0.14862\n",
    "                    'rank_score_srf_mistral',     ### 0.18041\n",
    "                    'rank_score_Linq_mistral',    ### 0.18217\n",
    "                    # 'rank_score_llm_merge', \n",
    "                    'rank_score_srf_mistral-WithAiresponse',\n",
    "                    'rank_score_Linq_mistral-WithAiresponse',\n",
    "                    'rank_score_srf_mistral-WithKeywords',\n",
    "                    'rank_score_Linq_mistral-WithKeywords',\n",
    "                    'rank_score_srf_mistral-WithKeywords-glm4',\n",
    "                    'rank_score_Linq_mistral-WithKeywords-glm4'\n",
    "                  ] \n",
    "# 确保数据类型正确，避免计算时出现类型错误\n",
    "df_test[q2p_scores_cols] = df_test[q2p_scores_cols].astype(float) \n",
    "# 计算加权得分\n",
    "df_test['rank_score_overall'] = np.sum(df_test[q2p_scores_cols].values * weights, axis=1) / np.sum(weights) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 82,
   "id": "d8c04e48-b440-4253-a8d2-dd1623dea8d2",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "### llm-overall \n",
    "df_test['rank_score_overall'] = df_test['rank_score_overall'].astype(float) \n",
    "test_candidates_list = df_test.sort_values(by=['index', 'rank_score_overall'], \n",
    "                                           ascending=[True, False]).groupby(by=['index'], as_index=True)['pids'].apply(list) \n",
    "test_candidates_list = test_candidates_list.apply(lambda x: ','.join(x[:20])) \n",
    "test_candidates_list.to_csv('my_data_overall.txt', sep='\\t', index=False, header=None) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 83,
   "id": "aec9d277-55d9-4be3-b3e8-89d8be0144d4",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1557550080"
      ]
     },
     "execution_count": 83,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "text/plain": [
       "9399951360"
      ]
     },
     "execution_count": 83,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "1557550080 ## qwen1.5-14b-gptq\n",
    "9399951360 ## glm4-9b"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 85,
   "id": "47424360-d4b7-47ca-a4fd-1700042c092f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Finish !!!\n"
     ]
    }
   ],
   "source": [
    "print('Finish !!!')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "095a2f5f-4148-441f-bdcb-fb33f2f6cbc3",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.14"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
